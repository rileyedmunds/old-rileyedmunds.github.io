<p>On December 15th, we concluded development of the TT4-Mini spherical tensegrity robot. Initially developed for planetary scouting, this smaller scale vehicle will be used as a test-bed for modeling robotic actuation and behavior across simulated planetary surfaces.</p>

<p>Here’s what we built…</p>

<ol>
  <li>
    <p>Controller</p>

    <ul>
      <li>Built ergonomic physical controller, future-proofed for future development (space for additional control modes)</li>
      <li>Integrated RF communication using XBee Wireless Modules in lieu of faulty Bluetooth connection.</li>
      <li>Rewrote control software for increased efficiency and reliability, emphasizing code modularity in preparation for ‘automatic’ control mode. </li>
    </ul>
  </li>
  <li>
    <p>Tensegrirty Structure</p>

    <ul>
      <li>Extended rod length from 10” to 12”.</li>
      <li>Designed new, sturdy motor covers.</li>
      <li>Upgraded from plastic endcaps to metallic endcaps.</li>
      <li>Designed and printed new spherical payload box structure.</li>
    </ul>
  </li>
  <li>
    <p>Receiver</p>

    <ul>
      <li>Refactored receiver code to efficiently parse and immediately execute motor commands received via RF.</li>
      <li>Designed new PCB to fit within spherical payload box.</li>
    </ul>
  </li>
</ol>

<p><br /></p>

<p><a href="https://www.youtube.com/watch?v=1IZeTiKoyLw&amp;vq=hd720" target="_blank">See our robot in action! (YOUTUBE)</a></p>

<p><img src="https://raw.githubusercontent.com/rileyedmunds/rileyedmunds.github.io/master/images/tensegrity/sidetable.JPG" alt="robot_side" /></p>

<p>Payload box PCB Schematics:</p>

<p><img src="https://raw.githubusercontent.com/rileyedmunds/rileyedmunds.github.io/master/images/tensegrity/schematic.JPG" alt="pcb schematic" /></p>

<p>Payload Box PCB:</p>

<p><img src="https://raw.githubusercontent.com/rileyedmunds/rileyedmunds.github.io/master/images/tensegrity/pcb.JPG" alt="slave_pcb" /></p>

<p>Physical controller prototype:</p>

<p><img src="https://raw.githubusercontent.com/rileyedmunds/rileyedmunds.github.io/master/images/tensegrity/controller.JPG" alt="controller prototype" /></p>

<p>Next semester I’m hoping to dive into development and testing of the ‘automatic’ control mode!</p>

<hr />

<p>Original Post: </p>

<p>Beginning in August, I’ve been working in a <a href="http://best.berkeley.edu/best-research/best-berkeley-emergent-space-tensegrities-robotics/">Robotics Lab</a>. I spend ~8 hours/week working on control systems , software, and animations.</p>

<p>Here are a couple things I’ve been working on and thinking about:</p>

<ol>
  <li>Applying reinforcement learning.</li>
</ol>

<p>Our lab’s collaborators at NASA Ames have developed a <a href="https://arxiv.org/abs/1609.09049">novel tweak on the guided policy search algorithm</a>, that meshes localized decisions and global decisions to learn a gait for the tensegrity.</p>

<p>The algorithm processes a very limited feature set: while the researchers attempted to collect more data, they ended up using 12 accelerometer measurements from each rod. I would like to propose attaching IMUs to each rod for far more comprehensive measurements and more holistsic feature selection.</p>

<ol>
  <li>Incorporating vision.</li>
</ol>

<p>As of this moment, the robot is controlled remotely by observing its current state and actuating motors according to which rods currently make up the base of the tensegrity and would most efficiently shift the center of mass ouside the base, and thus make the robot roll.</p>

<p>What if we could see in first person which rods form the base of the tensegrity? I propose including an fpv camera (within a gyroscope, to stay upright) within the payload box at the center of the robot. From there, we could write a simple image processing script to detect color-coded rods and thus infer which rods currently make up the base of the tensegrity.</p>

<p>For deployment of the robot in a real exploration scenario, first person vision seems necessary.</p>

<ol>
  <li>Distributing the nodes and relying on AI.</li>
</ol>

<p>If reinforcement learning can in fact learn gait in a 24-node tensegrity, what’s to say it can’t solve gait in a 500-node tensegrity? </p>

<p>Algorithms are often transferable, so once an optimization problem can be solved in one domain, its success in similar domains often requires little more than adjusted feature selection and tweaked settings. While this is clearly an oversimplification, I believe that solving and optimizing gait on one tensegrity opens the floodgates for research into other biologically-inspired robots that may consist of many more nodes.</p>
